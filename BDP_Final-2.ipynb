{"cells": [{"cell_type": "markdown", "id": "a6fe5d1b-2b00-420a-8100-9eae075b4bc5", "metadata": {}, "source": "### Part -2\n#### 1. Apply the filtering and feature reduction methods explored on initially sampled data.\n#### 2. Save the filtered and feature reduced data into new parquet files.\n#### 3. Make a new sample of 10k records from the filtered data for first hand analysis."}, {"cell_type": "code", "execution_count": 1, "id": "7713f43f-78d5-4f4a-9032-6ce69de26961", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:46:39) \n[GCC 10.4.0]\n3.1.3\n"}], "source": "#Ensure we are using the right kernel\nimport sys\nprint(sys.version)\nprint(spark.version)"}, {"cell_type": "code", "execution_count": 20, "id": "44a18751-f2a8-416c-9cc6-cc8a045a3bb0", "metadata": {}, "outputs": [], "source": "import time\nimport pyspark\nimport subprocess"}, {"cell_type": "markdown", "id": "a31eedfe-99ee-4b10-9629-51313f5899d2", "metadata": {}, "source": "#### Tuning Spark to increase the memory"}, {"cell_type": "code", "execution_count": 3, "id": "5d1604b4-7b27-497b-a4c0-80a7ca59f21b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Original spark.driver.maxResultSize: 1920m\nWaiting for 10 seconds for the enviroment to stop...\n"}, {"name": "stderr", "output_type": "stream", "text": "23/03/03 20:10:24 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n23/03/03 20:10:24 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n23/03/03 20:10:24 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n23/03/03 20:10:24 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"}, {"name": "stdout", "output_type": "stream", "text": "New spark.driver.maxResultSize: 8g\n"}], "source": "sc = spark.sparkContext\nprint('Original spark.driver.maxResultSize: ' + sc._conf.get('spark.driver.maxResultSize'))\n\n# Stop existing Spark environment\nsc.stop()\n\n# Waiting for the environment to stop\nsleep_time = 10\nprint(f'Waiting for {sleep_time} seconds for the enviroment to stop...')\ntime.sleep(sleep_time)\n\n# Applying new configuration and restarting Spark\nconf = pyspark.SparkConf().setAll([('spark.driver.maxResultSize', '8g')])\nsc = pyspark.SparkContext(conf=conf)\n\nprint('New spark.driver.maxResultSize: ' + sc._conf.get('spark.driver.maxResultSize'))\n\n# Starting  Spark session with configs applied\nspark = SparkSession(sc).builder.getOrCreate()"}, {"cell_type": "code", "execution_count": 4, "id": "0a794f3e-2940-4044-b3d3-6f3d1715bcc2", "metadata": {}, "outputs": [], "source": "import pandas as pd\nimport numpy as np\npd.set_option('display.max_colwidth', None)\npd.reset_option('display.max_rows')\nfrom itertools import compress \nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings(action='ignore')\nimport os\nimport shutil\n# import sh"}, {"cell_type": "code", "execution_count": 5, "id": "9f49096d-31f1-44ac-bd66-69c29af34624", "metadata": {}, "outputs": [], "source": "# !pip uninstall -y nltk\n# !pip install nltk --upgrade --no-cache-dir\n# %pip install nltk\u00a0-U"}, {"cell_type": "code", "execution_count": 6, "id": "a998b7f2-9aa5-49a2-bb04-c952f44692ec", "metadata": {}, "outputs": [], "source": "import nltk\n# nltk.download('popular', halt_on_error=False)"}, {"cell_type": "code", "execution_count": 7, "id": "7b6b191f-78ad-46dc-bfd9-b682d92ed185", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"}], "source": "import re\nfrom pyspark.ml.feature import MinHashLSH\nfrom pyspark.ml.feature import CountVectorizer,  IDF, CountVectorizerModel, Tokenizer, RegexTokenizer, StopWordsRemover\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import Row\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords"}, {"cell_type": "code", "execution_count": 8, "id": "abc1ed8f-8bb1-48cd-8e6a-56d9e7413c24", "metadata": {}, "outputs": [], "source": "# Display the spark DF in a beautified way\nspark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)\n\n## To use legacy casting notation for date\nspark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")"}, {"cell_type": "markdown", "id": "6cd27951-147b-4c34-995e-f20b323e12af", "metadata": {}, "source": "### Read the JSON files"}, {"cell_type": "code", "execution_count": 9, "id": "d318a426-f849-4b57-824f-967780d69992", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/03/03 20:13:43 WARN org.apache.spark.sql.execution.datasources.SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "CPU times: user 3.45 s, sys: 659 ms, total: 4.11 s\nWall time: 22min 56s\n"}, {"name": "stderr", "output_type": "stream", "text": "23/03/03 20:34:04 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"}], "source": "%time twitter_raw = spark.read.json('gs://msca-bdp-tweets/final_project')"}, {"cell_type": "markdown", "id": "9f744094-dba8-47d8-9213-ed436ec3a7ab", "metadata": {}, "source": "### Selecting the K-12/Education related tweets"}, {"cell_type": "code", "execution_count": 11, "id": "984ce945-a7e3-4dcc-ba43-82db2e31d2fd", "metadata": {}, "outputs": [], "source": "## Remove all special characters such as hastags, mentions, etc. \n\ntwit_filt = twitter_raw\\\n            .filter('user.followers_count > 0')\\\n            .filter('possibly_sensitive == FALSE or possibly_sensitive is NULL')\\\n            .filter('withheld_in_countries is NULL')\\\n            .filter('truncated == \"False\"')\\\n            .filter('lang == \"en\"')\\\n            .withColumn('text', lower('text'))\\\n            .withColumn('stripped_text', regexp_replace(col(\"text\"),\"[\\$#,&%\\\".]\",\"\"))"}, {"cell_type": "code", "execution_count": 12, "id": "dd43420c-89d2-46fb-88a6-2d02d97e541e", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "76837462"}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": "# Count of the dataframe with just textual filtering before filtering based on any keywords\ntwit_filt.count()"}, {"cell_type": "code", "execution_count": null, "id": "e0409308-0956-42cb-b08c-c63540a2e231", "metadata": {}, "outputs": [], "source": "twit_filt.describe()"}, {"cell_type": "code", "execution_count": 13, "id": "763ea4cb-b60d-4640-9ad8-4ef70cb47aae", "metadata": {}, "outputs": [], "source": "## Dictionary for words similar to 'education/K-12' \n\nedu_keywords = ['accountability', 'achievement', 'books', 'classes', 'colleges', 'commonly', 'community', 'curriculum', 'degree',\n                 'disabilities', 'district', 'districts', 'education', 'educational', 'elementary_school', 'found', 'grade', 'grades',\n                 'high_school', 'improve', 'include', 'k-12', 'k12', 'kindergarten', 'learning', 'level', 'local', 'math', \n                 'middle_school', 'parents', 'pre_school', 'primary_school', 'private', 'progress', 'public', 'requirements', \n                 'schools', 'science', 'scores', 'standardized', 'standards', 'state', 'states', 'students', 'systems', 'teachers', \n                 'tuition', 'united', 'years', 'Nursery', 'PhD', 'academic', 'academic conference', 'academic institution', \n                 'academic scholar', 'academic success', 'academic training', 'academics', 'bachelors', 'e learning', 'e-learning',\n                 'educational conference', 'educational research', 'educational stress', 'educationalist','grad', 'graduate', \n                 'home work', 'home-work', 'junior college', 'masters', 'masters degree', 'merit scholarships', 'nobel prize',\n                 'online education', 'p-12', 'p12','physical education', 'physics', 'post-doctorate', 'post-grad', 'post-graduate', \n                 'postdoctorate', 'postgrad', 'postgraduate', 'professional education', 'professor','research', 'research associate', \n                 'scholarships','study', 'study load', 'studying','syllabus', 'teaching', 'textbook','undergrad', 'undergraduate']\n\n\nremoval_words = ['guns', 'fashion', 'gaming', 'makeup', 'shooting', 'gun', 'kill', 'porn', 'sex', 'die', 'protest', 'tragedy', \n                 'killed', 'murder', 'uvalde', 'health', 'shoot', 'deceased', 'beauty', 'horny', 'shootings', 'gunned',\n                 'violen', 'ukraine', 'attack', 'dead', 'slaughter', 'crush', 'victim', 'massacre', 'trans', 'lgbt', 'threat', 'gay']\n\n\nregex_edu ='|'.join([\"(\" + c +\")\" for c in edu_keywords])\n\nregex_removal ='|'.join([\"(\" + c +\")\" for c in removal_words])\n\ntwit_filt = twit_filt.where(twit_filt['stripped_text'].rlike(regex_edu)).\\\nwhere(~twit_filt['stripped_text'].rlike(regex_removal))"}, {"cell_type": "code", "execution_count": null, "id": "e22cc92f-2aea-4ff8-b136-51b1145366d4", "metadata": {"scrolled": true, "tags": []}, "outputs": [], "source": "# Checking the words which had the most effect on filtering the tweet_text\n\nres = []\nfor word in edu_keywords:\n    count = twit_filt.filter('tweet_text like \"%' + word + '%\"').count()\n    res.append([word, count])\n\nres = sorted(res, key = lambda x:x[1], reverse = True)\nres[:20]"}, {"cell_type": "code", "execution_count": null, "id": "0891ac76-0728-4a45-850b-dc545f93255c", "metadata": {"scrolled": true, "tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "24550187"}, "execution_count": 14, "metadata": {}, "output_type": "execute_result"}], "source": "#count of the records after filtering based on keywords\ntwit_filt.count()"}, {"cell_type": "code", "execution_count": 16, "id": "95552d0f-a44e-4849-8f27-6e1145574638", "metadata": {}, "outputs": [], "source": "@udf\ndef get_importance(text):\n    global edu_keywords\n    words = text.split()\n    total_count = 0\n    for i in edu_keywords:\n        occurance_count = words.count(i)\n        total_count += occurance_count\n\n    if(total_count > 1):\n        return 1\n    else:\n        return 0\n\n#On original 24.4 Million\ntwit_filt_filt = twit_filt.withColumn(\"important\", get_importance(\"stripped_text\"))"}, {"cell_type": "code", "execution_count": 17, "id": "df3b4f58-190c-48f1-998a-e2466abc0df4", "metadata": {"scrolled": true, "tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 10:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "CPU times: user 2.75 s, sys: 849 ms, total: 3.6 s\nWall time: 11min 55s\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "4814661"}, "execution_count": 17, "metadata": {}, "output_type": "execute_result"}], "source": "%%time\ntwit_filt_filt = twit_filt_filt.filter(\"important == 1\")\ntwit_filt_filt.count()"}, {"cell_type": "code", "execution_count": null, "id": "fe60a720-6384-4178-b0ac-fdfeb5c54b53", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "#save the filtered file into a parquet\ntwit_filt_filt.write.format(\"parquet\").\\\nmode('overwrite').\\\nsave('gs://msca-bdp-students-bucket/shared_data/saikrishnaj/keyword_filtered_data')"}, {"cell_type": "code", "execution_count": 21, "id": "085d8b26-a1c6-4c6a-8150-b1f359d48b2c", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "8.9 G  8.9 G  gs://msca-bdp-students-bucket/shared_data/saikrishnaj/keyword_filtered_data\n\n"}], "source": "path = 'gs://msca-bdp-students-bucket/shared_data/saikrishnaj/keyword_filtered_data/'\ncmd = 'hadoop fs -du -s -h ' + path\n\np = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\nfor line in p.stdout.readlines():\n    print (line)\n    \nretval = p.wait()"}, {"cell_type": "code", "execution_count": null, "id": "3f495588-a29f-44ed-87db-5635ce28d0cf", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}